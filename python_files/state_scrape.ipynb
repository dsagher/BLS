{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "NOW = dt.datetime.now().strftime('%d-%b-%Y_%H:%M:%S')\n",
    "PATH = '/Users/danielsagher/Dropbox/Documents/projects/bls_api_project/'\n",
    "\n",
    "bls_state_url = 'https://data.bls.gov/cgi-bin/surveymost?sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from bls main state unemployment site\n",
    "\n",
    "bls_ro = rq.get(bls_state_url)\n",
    "bls_ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn it into beautiful soup using BS4\n",
    "\n",
    "ro_soup = soup(bls_ro.text, 'html.parser')\n",
    "ro_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the tags containing links and names of states\n",
    "\n",
    "state_links = ro_soup.css.select('a[href^=\"https://data.bls.gov/cgi-bin/surveymost?\"]')\n",
    "state_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension to make a list of links next to their respective states\n",
    "\n",
    "state_link_list = [(state_links[i].get('href'), state_links[i].get_text()) for i in range(len(state_links))][1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define web-scraper\n",
    "\n",
    "def state_scraper(state_link_list):\n",
    "\n",
    "    '''\n",
    "    This function scrapes the BLS State Employment page for all Series and SeriesID's \n",
    "    and deposits them into a Pandas DataFrame. \n",
    "    The result is then automatically saved to a CSV. \n",
    "    '''\n",
    "    final_df = pd.DataFrame([])\n",
    "\n",
    "    for link in range(len(state_link_list)):\n",
    "\n",
    "        try:\n",
    "            # Response from link\n",
    "            print(f'Initializing \"get\" request for {state_link_list[link][1]} ')\n",
    "            state_ro = rq.get(state_link_list[link][0]) \n",
    "            print(f'State Link for {state_link_list[link][1]} Acquired')\n",
    "\n",
    "            # Soup\n",
    "            state_soup = soup(state_ro.text, 'html.parser')\n",
    "\n",
    "            # Find serials and survey names\n",
    "            state_serial_and_id = state_soup.find_all('dt')[0].get_text()\n",
    "\n",
    "            # Split into list\n",
    "            state_serial_and_id = state_serial_and_id.split('\\n')[1:-1]\n",
    "\n",
    "            # Turn into DataFrame, split by '-'\n",
    "            df = pd.DataFrame([item.split(' - ') for item in state_serial_and_id], columns=['series', 'seriesID'])\n",
    "\n",
    "            # Take state name from series column and put it in its own column\n",
    "            df['state'] = df['series'].str[0:(df['series'].str.find(',').astype(int)[0])]\n",
    "            df['series'] = df['series'].str[(df['series'].str.find(', ').astype(int)[0]+2):]\n",
    "            final_df['survey'] = 'CES'\n",
    "\n",
    "            # Append to final DataFrame\n",
    "            final_df = pd.concat([final_df, df], ignore_index= True)\n",
    "\n",
    "            # Be nice to BLS\n",
    "            print(f'{state_link_list[link][1]} data added to DataFrame')\n",
    "            print('Sleeping for 2 seconds')\n",
    "            \n",
    "            time.sleep(2)\n",
    "            clear_output()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {state_link_list[link][1]}: {e}\")\n",
    "\n",
    "    final_df.to_csv(PATH + f'outputs/state_scrape_op/state_series_dimension_{NOW}.csv', index=False)\n",
    "    \n",
    "    return final_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it\n",
    "\n",
    "state_scraper(state_link_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
